## คู่มือสั้นๆ สำหรับน้องใหม่: ระบบ Knowledge Graph + Hybrid Search (กฎหมายแรงงานไทย)

เอกสารนี้อธิบายแบบเข้าใจง่าย ว่าแต่ละส่วนทำอะไร ทำงานร่วมกันอย่างไร และคำศัพท์สำคัญที่ต้องรู้ เพื่อให้ต่องานได้เร็ว

### ภาพรวมสถาปัตยกรรม (Architecture)

- **Frontend (Next.js)**
  - หน้าแบบฟอร์มคำร้อง ส่งคำค้นไป backend เพื่อ “เดาความหมาย” และแนะนำหัวข้อเอกสารศาล
  - API proxy ที่ `app/api/court-documents/route.ts` ส่งต่อไป backend
- **Backend (FastAPI)**
  - เสิร์ชแบบ Hybrid: Vector Embeddings + TF‑IDF + Knowledge Graph (Neo4j)
  - Endpoint สำคัญ: `/court-documents/search`, `/search`, `/answer`, `/ingest`
- **Database (Neo4j)**
  - เก็บ Knowledge Graph (โหนด/ความสัมพันธ์) และชิ้นข้อความ `DocChunk`

โฟลว์หลัก: ผู้ใช้พิมพ์ข้อความ → Backend ค้นเอกสารและข้อมูลจากกราฟ → สร้าง “คำแนะนำเอกสารศาล” → ส่งกลับไปแสดงในฟรอนต์

---

### ไฟล์/โมดูลสำคัญ และหน้าที่

#### 1) `src/api/routes.py` (FastAPI Routes)

- `POST /ingest`: รับข้อความหลายชิ้น → ดึงความรู้แบบ rule‑based → อัปเสิร์ทลง Neo4j และเก็บ `DocChunk`
- `GET /search`: ค้นเอกสาร + ดึง facts จากกราฟ (ใช้ `hybrid_search`)
- `GET /answer` และ `POST /ask`: สร้างคำตอบสรุปจากผลค้นหาและ facts
- `GET /court-documents/search`: ใช้ Hybrid (เวกเตอร์+TF‑IDF+KG) เพื่อ “เดา” เทมเพลตเอกสารศาล เช่น “คำฟ้องคดีแรงงาน รง1 / ศาลแรงงานกลาง” และมี fuzzy fallback ถ้าหาไม่เจอจาก KG
- ตัวช่วยอื่นๆ: เก็บ case ล่าสุดไว้ใน buffer, `/Health` สำหรับตรวจสุขภาพระบบ

สิ่งที่ควรรู้: เราคงรูปแบบผลลัพธ์ให้ฟรอนต์ใช้งานง่าย `{ query, case_id, results: [...], total }` และมี heuristic mapping ง่ายๆ แปลงคำค้น/ข้อเท็จจริง → ชื่อเอกสารศาล

#### 2) `src/services/extraction.py` (Rule‑based Extraction → Knowledge Graph)

- `rule_based_extract(text)`: อ่านข้อความไทย → ตรวจจับ โจทก์/จำเลย, สัญญาจ้าง, จำนวนเงิน (บาท), วันที่ (ไทย), หมวด/มาตรา ฯลฯ → สร้างโหนด (`SimpleNode`) และความสัมพันธ์ (`SimpleRel`)
- `detect_case_id(texts)`: หา/สร้างรหัสคดีอัตโนมัติจากข้อความ

สรุป: ดึง “โครงสร้างความรู้” จากข้อความธรรมดา ให้ Neo4j เอาไปใช้งานได้

#### 3) `src/services/neo4j_service.py` (เชื่อมต่อ Neo4j)

- `setup_constraints()`: สร้าง unique constraints ที่จำเป็น (กันข้อมูลซ้ำ)
- `upsert_graph(graph_docs, case_id)`: สร้าง/เชื่อมโหนดกับความสัมพันธ์ลง Neo4j + ผูกกับคดี
- `index_doc_chunks(text_chunks, case_id)`: เก็บชิ้นข้อความแต่ละหน้า/ย่อหน้า เป็นโหนด `DocChunk`
- `fetch_doc_chunks(case_id)`: ดึง `DocChunk` สำหรับค้นหา
- `graph_retrieve(case_id, limit)`: ดึง facts จากกราฟ (บุคคล/บทบาท, วันที่, จำนวนเงิน, มาตรา ฯลฯ)

สรุป: เป็น “สะพาน” ระหว่างโค้ด Python กับฐาน Neo4j

#### 4) `src/services/search.py` (Hybrid Search)

- ส่วน TF‑IDF ดั้งเดิม: `build_tfidf`, `vectorize_query`, `cosine` → ให้คะแนนความใกล้เคียงแบบ keyword
- ส่วน Vector Embeddings (ถ้ามีคีย์ OpenAI): `_embed_texts`, `_embed_query` → สร้างเวกเตอร์ความหมายด้วย `OpenAIEmbeddings`
- ไฮบริดตัวจริง: `hybrid_search(query, case_id, k)`
  - คำนวณคะแนน TF‑IDF และ Vector (ถ้ามี)
  - รวมคะแนน: 0.7 เวกเตอร์ + 0.3 TF‑IDF (ถ้าไม่มีเวกเตอร์ ใช้ TF‑IDF อย่างเดียว)
  - ดึง facts จากกราฟด้วย `graph_retrieve`
  - ส่ง top docs + facts กลับไปให้ API

สรุป: ค้นแบบเข้าใจความหมาย (เวกเตอร์) + คำตรง (TF‑IDF) + อ้างอิงกราฟ (facts)

#### 5) `src/config.py` (ตั้งค่า และศัพท์ที่ใช้)

- ตัวแปรแวดล้อม Neo4j: `NEO4J_URI`, `NEO4J_USER`, `NEO4J_PASSWORD`, `NEO4J_DATABASE`
- สคีมากราฟ: `ALLOWED_NODE_LABELS`, `ALLOWED_REL_TYPES`
- ค่าอื่นๆ: `MAX_VOCAB_SIZE`, ข้อความบรรยาย API

#### 6) `src/utils/thai_parser.py` (ยูทิลภาษาไทย)

- ช่วย normalize ตัวเลขไทย, ตัดคำ, แปลงวันที่ไทยเป็น ISO, ดึงจำนวนเงิน ฯลฯ

---

### คำศัพท์สำคัญ (ภาษาคน)

- **Knowledge Graph (KG)**: ฐานข้อมูลเป็น “จุด (โหนด)” และ “เส้นเชื่อม (ความสัมพันธ์)” เช่น โจทก์ → เป็นคู่ความ → คดี
- **Ontology**: แบบแผนชนิดโหนด/ความสัมพันธ์ที่อนุญาต (เช่น Person, CourtCase, Date)
- **DocChunk**: ชิ้นส่วนข้อความของเอกสาร (หน้า/ย่อหน้า) ที่เอามาค้นหาได้
- **TF‑IDF**: วัดความใกล้เคียงจาก “คำที่เหมือนกัน” เหมาะกับค้นคีย์เวิร์ดตรงๆ
- **Embedding/Vector**: ตัวเลขแทน “ความหมาย” ของประโยค/คำ ทำให้ค้นคำพ้อง/พิมพ์ไม่ตรงได้ดี
- **Hybrid Search**: ผสม เวกเตอร์ + TF‑IDF + ข้อเท็จจริงจากกราฟ ให้คำตอบทั้ง “ใกล้ความหมาย” และ “ตรวจสอบได้”

---

### การทำงานของ `GET /court-documents/search`

1. รับ `q` (คำค้น) และ `case_id` (ไม่ใส่ก็ใช้คดีล่าสุด)
2. เรียก `hybrid_search` → ได้เอกสารที่ใกล้เคียง + facts จากกราฟ
3. ใช้กติกาง่ายๆ แมปเป็นคำแนะนำเอกสารศาล เช่น พิมพ์ “เลิกจ้างไม่เป็นธรรม” → แนะนำ “คำฟ้องคดีแรงงาน รง1 / ศาลแรงงานกลาง”
4. ถ้าเดาไม่ได้ → fallback ไป fuzzy list

ผลลัพธ์คงรูปแบบเดิม เพื่อให้ฟรอนต์ไม่ต้องแก้

---

### ตั้งค่าและรัน (Local)

1. ตั้งค่า Neo4j (`NEO4J_URI`, `NEO4J_USER`, `NEO4J_PASSWORD`)
2. (ทางเลือก) ตั้ง `OPENAI_API_KEY` ถ้าต้องใช้เวกเตอร์จาก OpenAI
3. รัน backend:
   ```bash
   python backend/NEO4j/run_api.py
   ```
4. ทดสอบ:
   ```bash
   curl 'http://localhost:8000/court-documents/search?q=เลิกจ้างไม่เป็นธรรม'
   ```

หมายเหตุ: ถ้าไม่มี `OPENAI_API_KEY` ระบบจะยังใช้ TF‑IDF ได้ตามปกติ (แค่ไม่ใช้งานเวกเตอร์)

---

### เคล็ดลับสำหรับการต่อยอด

- ต้องการค้นเร็ว/แม่นในดาต้าใหญ่: เพิ่ม embedding index จริง (Neo4j vector index หรือ Qdrant)
- อยากแยกเอนทิตีดีขึ้น: เติมกฎใน `extraction.py` หรือใช้ NER/LLM ช่วย
- อยากเพิ่มชนิดเอกสารศาล: แก้ heuristic ใน `/court-documents/search` ให้รองรับคำใหม่ๆ
- อยาก debug ผลกราฟ: ใช้ Neo4j Browser ดูโหนด/เส้นเชื่อมแบบภาพ

---

### สรุปสั้น (จำไว้)

- ฟรอนต์พิมพ์ข้อความ → Backend `hybrid_search` → ผสม Vector + TF‑IDF + KG facts → แนะนำเอกสารศาล
- ไม่มีคีย์ OpenAI ก็รันได้ (ใช้ TF‑IDF อย่างเดียว)
- ทุกอย่างผูกกับ Neo4j เป็นฐานความรู้กลางของระบบ



